print(z_scores)
cat("\nP-value:\n")
print(p_values)
setwd('C:\\Users\\fandj\\Downloads')
# 1. Импорт данных
a <- read.csv(file="rand2.csv", header=TRUE, sep=",")
# Проверяем на пропущенные значения (NA)
print(colSums(sapply(a, is.na)))
# 2. Создание subset
subset_ijd <- subset(a, color %in% c("I", "J", "D"))
print(colSums(sapply(subset_ijd, is.na)))
head(subset_ijd)
dim(subset_ijd)
# 3. Линейная регрессия
model <- lm(price ~ carat, data=subset_ijd)
summary(model)
# 4. Подготовка новых значений веса
max_carat <- max(subset_ijd$carat)
new_weights <- max_carat * c(1.05, 1.10, 1.15)
print(new_weights)
# 5. Прогнозы
point_predictions <- predict(model, data.frame(carat=new_weights))
print(point_predictions)
predictive_intervals <- predict(model, data.frame(carat=new_weights), interval="prediction")
print(predictive_intervals)
confidence_intervals <- predict(model, newdata=data.frame(carat=new_weights), interval="confidence")
print(confidence_intervals)
# 6. Визуализация
library(ggplot2)
# Построение регрессионной прямой
plot_data <- data.frame(carat=subset_ijd$carat, price=subset_ijd$price)
ggplot(plot_data, aes(x=carat, y=price)) +
geom_point() +
geom_smooth(method="lm", se=TRUE, color="blue", fill="lightblue") +
labs(title="Линейная регрессия: цена алмаза vs вес",
x="Вес (carat)",
y="Цена (price)") +
theme_minimal()
# 1. Загрузка необходимых библиотек
library(ggplot2)
# 2. Создание модели линейной регрессии (предполагаем, что модель уже создана)
# Подготовка данных для графика
# 2. Создание модели линейной регрессии (предполагаем, что модель уже создана)
# Подготовка данных для графика
# Добавляем предсказания и интервалы в исходные данные
subset_ijd$predicted <- predict(model, newdata = subset_ijd)
subset_ijd$lower_prediction <- predict(model, newdata = subset_ijd, interval="prediction")[, 2]
subset_ijd$upper_prediction <- predict(model, newdata = subset_ijd, interval="prediction")[, 3]
subset_ijd$lower_confidence <- predict(model, newdata = subset_ijd, interval="confidence")[, 2]
subset_ijd$upper_confidence <- predict(model, newdata = subset_ijd, interval="confidence")[, 3]
# 3. Добавление продолжения линии до максимального нового веса
new_weights_extended <- seq(min(subset_ijd$carat), max(new_weights), length.out = 100)  # Расширяем диапазон до max нового веса
predictions_extended <- predict(model, newdata = data.frame(carat = new_weights_extended))  # Прогнозируем для новых значений
# 4. Построение графика
ggplot(subset_ijd, aes(x = carat, y = price)) +
geom_point(aes(color = "data"), size = 2) +  # Точки исходных данных
geom_line(aes(y = predicted), color = "blue", size = 1) +  # Линия линейной регрессии для исходных данных
geom_ribbon(aes(ymin = lower_prediction, ymax = upper_prediction, fill = "prediction interval"), alpha = 0.5) +  # Предиктивный интервал
geom_ribbon(aes(ymin = lower_confidence, ymax = upper_confidence, fill = "confidence interval"), alpha = 0.5) +  # Доверительный интервал
geom_point(data = data.frame(carat = new_weights, price = point_predictions), aes(x = carat, y = price, color = "NewWeights"), size = 3) +  # Новые точки
geom_line(data = data.frame(carat = new_weights_extended, price = predictions_extended), aes(x = carat, y = price), color = "blue", linetype = "dashed") +  # Продолжение линии до максимального нового веса
labs(
title = "Linear regression with predicted and confidence intervals",
x = "Carat (weight)",
y = "Price (price)",
color = "Data type",
fill = "interval type"
) +
scale_color_manual(values = c("data" = "black", "NewWeights" = "red")) +  # Цвета точек
scale_fill_manual(values = c("prediction interval" = "lightblue", "confidence interval" = "lightgreen")) +  # Цвета интервалов
theme_minimal()
setwd("D:\\Coding\\math\\DS\\r\\3_lab")
data <- read.table(file="moscow.txt", header = TRUE)
summary(data)
dim(data)
set.seed(42)
library(nnet)
library(ggplot2)
library(dplyr)
library(caret)
data$class <- cut(data$totsp, breaks = 4, labels = c("small", "medium", "large", "huge"))
train_index <- caret::createDataPartition(data$class, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
validation_index <- createDataPartition(temp_data$class, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
cat("Size of train data:", nrow(train_data), "\n")
cat("Size of validation data:", nrow(validation_data), "\n")
cat("Size of test data:", nrow(test_data), "\n")
model <- multinom(class ~ ., data = train_data)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
model_summary <- summary(model)
# Коэффициенты модели
coefs <- model_summary$coefficients
# Стандартные ошибки
std_errors <- model_summary$standard.errors
# Z-статистика
z_values <- coefs / std_errors
# P-значения
p_values <- 2 * (1 - pnorm(abs(z_values)))
# Вывод результатов
results <- cbind(coefs, std_errors, z_values, p_values)
colnames(results) <- c("Estimate", "Std. Error", "Z value", "Pr(>|z|)")
results
logLik(model)
# AIC
AIC(model)
# Оценки R^2 (для multinom это не стандартная метрика, но ее аналоги могут быть рассчитаны через псевдо-R^2):
library(DescTools)
install.packages("DescTools")
library(DescTools)
PseudoR2(model, which = "McFadden")
model <- multinom(class ~ nrooms, data = train_data)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
install.packages("stargazer")
library(stargazer)
stargazer(model, type = "text")
model <- train(Species ~ ., data = train_data, method = "multinom", trace = FALSE)
model <- caret::train(Species ~ ., data = train_data, method = "multinom", trace = FALSE)
model <- train(class ~ ., data = train_data, method = "multinom", trace = FALSE)
summary(model)
setwd("D:\\Coding\\math\\DS\\r\\3_lab")
data <- read.table(file="moscow.txt", header = TRUE)
summary(data)
dim(data)
set.seed(42)
library(nnet)
library(ggplot2)
library(dplyr)
library(caret)
data$class <- cut(data$totsp, breaks = 4, labels = c("small", "medium", "large", "huge"))
train_index <- caret::createDataPartition(data$class, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
validation_index <- createDataPartition(temp_data$class, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
cat("Size of train data:", nrow(train_data), "\n")
cat("Size of validation data:", nrow(validation_data), "\n")
cat("Size of test data:", nrow(test_data), "\n")
model <- multinom(class ~ ., data = train_data)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
# Логарифм правдоподобия
logLik(model)
# AIC
AIC(model)
model <- train(class ~ ., data = train_data, method = "multinom", trace = FALSE)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
install.packages("stargazer")
library(stargazer)
stargazer(model, type = "text")
install.packages("stargazer")
setwd("D:\\Coding\\math\\DS\\r\\3_lab")
data <- read.table(file="moscow.txt", header = TRUE)
summary(data)
dim(data)
set.seed(42)
library(nnet)
library(ggplot2)
library(dplyr)
library(caret)
data$class <- cut(data$totsp, breaks = 4, labels = c("small", "medium", "large", "huge"))
train_index <- caret::createDataPartition(data$class, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
validation_index <- createDataPartition(temp_data$class, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
cat("Size of train data:", nrow(train_data), "\n")
cat("Size of validation data:", nrow(validation_data), "\n")
cat("Size of test data:", nrow(test_data), "\n")
model <- train(class ~ ., data = train_data, method = "multinom", trace = FALSE)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
# Логарифм правдоподобия
logLik(model)
# AIC
AIC(model)
model <- carret::train(class ~ ., data = train_data, method = "multinom", trace = FALSE)
model <- caret::train(class ~ ., data = train_data, method = "multinom", trace = FALSE)
summary(model)
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
# Логарифм правдоподобия
logLik(model)
setwd("D:\\Coding\\math\\DS\\r\\3_lab")
data <- read.table(file="moscow.txt", header = TRUE)
summary(data)
dim(data)
set.seed(42)
library(nnet)
library(ggplot2)
library(dplyr)
library(caret)
data$class <- cut(data$totsp, breaks = 4, labels = c("small", "medium", "large", "huge"))
train_index <- caret::createDataPartition(data$class, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
validation_index <- createDataPartition(temp_data$class, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
cat("Size of train data:", nrow(train_data), "\n")
cat("Size of validation data:", nrow(validation_data), "\n")
cat("Size of test data:", nrow(test_data), "\n")
model <- caret::train(class ~ ., data = train_data, method = "multinom", trace = FALSE)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
# Результаты
cat("Z-score:\n")
print(z_scores)
cat("\nP-value:\n")
print(p_values)
# Логарифм правдоподобия
logLik(model)
model
#library(ggplot2)
library(caret)
library(kernlab)
setwd("D:\\Coding\\math\\DS\\r\\3_lab")
data <- read.table(file="moscow.txt", header = TRUE)
summary(data)
dim(data)
set.seed(42)
library(nnet)
library(ggplot2)
library(dplyr)
library(caret)
data$class <- cut(data$totsp, breaks = 4, labels = c("small", "medium", "large", "huge"))
train_index <- caret::createDataPartition(data$class, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
validation_index <- createDataPartition(temp_data$class, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
cat("Size of train data:", nrow(train_data), "\n")
cat("Size of validation data:", nrow(validation_data), "\n")
cat("Size of test data:", nrow(test_data), "\n")
data <- data[, data$totsp]
data <- select(data, -totsp)
dim(data)
summary(data)
dim(data)
setwd("D:\\Coding\\math\\DS\\r\\3_lab")
data <- read.table(file="moscow.txt", header = TRUE)
summary(data)
dim(data)
set.seed(42)
library(nnet)
library(ggplot2)
library(dplyr)
library(caret)
print(colSums(sapply(data, is.na)))
data$class <- cut(data$totsp, breaks = 4, labels = c("small", "medium", "large", "huge"))
data <- select(data, -totsp)
dim(data)
summary(data)
train_index <- caret::createDataPartition(data$class, p = 0.6, list = FALSE)
train_data <- data[train_index, ]
temp_data <- data[-train_index, ]
validation_index <- createDataPartition(temp_data$class, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_index, ]
test_data <- temp_data[-validation_index, ]
cat("Size of train data:", nrow(train_data), "\n")
cat("Size of validation data:", nrow(validation_data), "\n")
cat("Size of test data:", nrow(test_data), "\n")
cat("Size of train data:", dim(train_data), "\n")
cat("Size of validation data:", dim(validation_data), "\n")
cat("Size of test data:", dim(test_data), "\n")
model <- multinom(class ~ ., data = train_data)
summary(model)
# Оценка модели на валидационной выборке
validation_predictions <- predict(model, validation_data)
str(validation_predictions)
print(validation_predictions)
head(validation_predictions)
validation_pred_prob <- predict(model, validation_data, type="prob")
print(validation_pred_prob)
head(validation_pred_prob)
misclassification=table(predict(model), validation_data$class)
misclassification=table(validation_predictions, validation_data$class)
misclassification
correct_predictions <- sum(diag(misclassification))
total_predictions <- sum(misclassification)
accuracy <- correct_predictions / total_predictions
print(round(accuracy * 100 , 2), "%")
correct_predictions <- sum(diag(misclassification))
# Общее количество предсказаний
total_predictions <- sum(misclassification)
# Вычисление точности
accuracy <- correct_predictions / total_predictions
# Вывод точности
print(paste("Точность модели:", round(accuracy * 100, 2), "%"))
validation_accuracy <- mean(validation_predictions == validation_data$class)
cat("validation accuracy:", validation_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
cat("\nP-value:\n")
print(p_values)
# AIC
AIC(model)
model <- multinom(class ~ nrooms + livesp, data = train_data)
summary(model)
validation_predictions <- predict(model, validation_data)
validation_pred_prob <- predict(model, validation_data, type="prob")
head(validation_predictions)
head(validation_pred_prob)
misclassification=table(validation_predictions, validation_data$class)
misclassification
correct_predictions <- sum(diag(misclassification))
# Общее количество предсказаний
total_predictions <- sum(misclassification)
# Вычисление точности
accuracy <- correct_predictions / total_predictions
# Вывод точности
print(paste("Точность модели:", round(accuracy * 100, 2), "%"))
# Финальная оценка на тестовой выборке
test_predictions <- predict(model, test_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors  # Стандартные ошибки
# Вычисление z-статистик и p-значений
z_scores <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(z_scores)))
cat("\nP-value:\n")
print(p_values)
# AIC
AIC(model)
test_predictions <- predict(model, validation_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
test_predictions <- predict(model, validation_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
test_predictions <- predict(model, validation_data)
test_accuracy <- mean(test_predictions == test_data$class)
cat("test data accuracy:", test_accuracy, "\n")
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients[, "Estimate"]  # Коэффициенты модели
str(coef_summary)
coef_summary <- summary(model)
# Извлечение коэффициентов
coefficients <- coef_summary$coefficients
# Добавление имен колонок
colnames(coefficients) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
dim(coefficients)
print(coefficients)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
coef_summary <- summary(model)
coefficients <- coef_summary$coefficients  # Коэффициенты модели
std_errors <- coef_summary$standard.errors
estimates <- coefficients
std_errors <- std_errors
# Уровень доверия
alpha <- 0.05
z_alpha_over_2 <- qnorm(1 - alpha / 2)  # Критическое значение z
# Вычисление доверительных интервалов
lower_bounds <- estimates - z_alpha_over_2 * std_errors
upper_bounds <- estimates + z_alpha_over_2 * std_errors
# Создание таблицы доверительных интервалов
confidence_intervals <- data.frame(
Coefficient = estimates,
Lower_Bound = lower_bounds,
Upper_Bound = upper_bounds
)
# Вывод доверительных интервалов
print(confidence_intervals)
estimates <- coefficients
std_errors <- std_errors
# Уровень доверия
alpha <- 0.05
z_alpha_over_2 <- qnorm(1 - alpha / 2)  # Критическое значение z
# Вычисление доверительных интервалов
lower_bounds <- estimates - z_alpha_over_2 * std_errors
upper_bounds <- estimates + z_alpha_over_2 * std_errors
# Создание таблицы доверительных интервалов
confidence_intervals <- data.frame(
Coefficient = estimates,
Lower_Bound = lower_bounds,
Upper_Bound = upper_bounds
)
# Вывод доверительных интервалов
print(confidence_intervals)
